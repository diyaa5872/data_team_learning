{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54498f9a-c84f-4e93-aee0-e17a55fa184c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Created a table with some data in DELTA format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404ce8cb-e336-4b32-8308-d73625faf19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "data = [\n",
    "    (1, 'Alice', 'HR', '2023-01-01'),\n",
    "    (2, 'Bob', 'Finance', '2023-01-01'),\n",
    "    (3, 'Charlie', 'IT', '2023-01-01')\n",
    "]\n",
    "\n",
    "columns = ['emp_id', 'name', 'department', 'start_date']\n",
    "\n",
    "df = spark.createDataFrame(data, columns) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"string\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employee_dim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f440ce-d15c-46ed-beb2-20337fa2121c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8b3cee7-f544-4a7b-9177-81ccca4fdefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "data with some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4434be-86e1-402c-9c1d-c504f5f1a287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incoming_data = [\n",
    "    (1, 'Alice', 'HR'),             # No change\n",
    "    (2, 'Bob', 'Marketing'),        # Changed department\n",
    "    (4, 'David', 'IT')              # New employee\n",
    "]\n",
    "\n",
    "incoming_columns = ['emp_id', 'name', 'department']\n",
    "incoming_df = spark.createDataFrame(incoming_data, incoming_columns)\n",
    "\n",
    "display(incoming_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6048790e-8e64-4d08-b4c2-501aa7f2c255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"delta\").table(\"default.employee_dim\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7cd1581-7870-422d-b878-9028c0702eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "applying left join from incoming data to current data to check the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a6321b-8176-4a75-b792-f0b990990b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "joined_df=incoming_df.alias(\"incoming\").join(\n",
    "  df.filter(\"is_current=True\").alias(\"current\"),\n",
    "  col(\"incoming.emp_id\") == col(\"current.emp_id\"),\n",
    "  \"left\"\n",
    ")\n",
    "\n",
    "display(joined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f28a4ddb-6a5b-4608-9b74-e7692dacb5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filtering the data-\n",
    "- if the name is NULL in current data, means new data\n",
    "- if the names are not same after joining, means name updated\n",
    "- if the department name is not same, means department updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b181e4d7-4041-4cd6-ab0c-36f2228fac09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filtered out the changes\n",
    "changed_records = joined_df.filter(\n",
    "    (col(\"current.name\").isNull()) |\n",
    "    (col(\"incoming.name\") != col(\"current.name\")) |\n",
    "    (col(\"incoming.department\") != col(\"current.department\"))\n",
    ").select(\"incoming.*\")\n",
    "\n",
    "display(changed_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c09d2d-029f-443f-a725-23ed41432e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Expire old records\n",
    "updates_df = changed_records.join(\n",
    "    df.filter(\"is_current = True\"), \"emp_id\"\n",
    ").select(\n",
    "    \"emp_id\", \"name\", \"department\", \"start_date\",\n",
    "    current_date().alias(\"end_date\"),\n",
    "    lit(False).alias(\"is_current\")\n",
    ")\n",
    "\n",
    "# Add new records with current flag\n",
    "new_records_df = changed_records.withColumn(\"start_date\", current_date()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "display(new_records_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8451e567-a92e-4262-b663-338ebe667c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view for the current dimension table\n",
    "df.createOrReplaceTempView(\"employee_dim_view\")\n",
    "\n",
    "# Create a temporary view for the new records to be inserted\n",
    "new_records_df.createOrReplaceTempView(\"new_records_view\")\n",
    "\n",
    "# Expire old records by updating end_date and is_current for matched records\n",
    "expired_records = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    d.emp_id,\n",
    "    d.name,\n",
    "    d.department,\n",
    "    d.start_date,\n",
    "    n.start_date AS end_date,\n",
    "    false AS is_current\n",
    "  FROM employee_dim_view d\n",
    "  INNER JOIN new_records_view n\n",
    "    ON d.emp_id = n.emp_id\n",
    "  WHERE d.is_current = true\n",
    "\"\"\")\n",
    "\n",
    "# Keep unchanged current records\n",
    "unchanged_records = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM employee_dim_view\n",
    "  WHERE is_current = true\n",
    "    AND emp_id NOT IN (SELECT emp_id FROM new_records_view)\n",
    "\"\"\")\n",
    "\n",
    "# Combine expired, unchanged, and new records for the new SCD2 view\n",
    "scd2_final = expired_records.unionByName(unchanged_records).unionByName(new_records_df)\n",
    "\n",
    "# Create or replace a view with the SCD2 result\n",
    "scd2_final.createOrReplaceTempView(\"employee_dim_scd2_view\")\n",
    "\n",
    "display(spark.sql(\"SELECT * FROM employee_dim_scd2_view\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad3c771-ec6b-43a4-9b5c-4be8e745c03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5050786455391749,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Slowly-changing-dimension-2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
