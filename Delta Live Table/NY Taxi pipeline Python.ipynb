{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c87d2c-79b2-4e9c-9b90-9a513c52843c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "057bff40-6751-44ca-b375-27285a7aec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creation of bronze streaming table \n",
    "@dlt.table (\n",
    "    name=\"taxi_raw_records\",\n",
    "    comment=\"Bronze layer: Raw data ingestion from samples\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_distance\",\"trip_distance>0.0\")\n",
    "def taxi_raw_records():\n",
    "    return (\n",
    "        spark.readStream #readstream bcz wanted to create a streaming table\n",
    "        .format(\"delta\")  # Because samples.nyctaxi.trips is a Delta table\n",
    "        .table(\"samples.nyctaxi.trips\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d3b353-2757-49a6-a49d-b51829a1c1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silver flagged table code \n",
    "@dlt.table (\n",
    "    name=\"flagged_rides\",\n",
    "    comment=\"Silver flagged layer: Rides with either suspicious fare or short trip but high fare\"\n",
    ")\n",
    "def flagged_rides():\n",
    "    df = dlt.read_stream(\"taxi_raw_records\")\n",
    "\n",
    "    return (\n",
    "        df.filter(\n",
    "            (\n",
    "                (col(\"pickup_zip\") == col(\"dropoff_zip\")) & (col(\"fare_amount\") > 50)\n",
    "            ) |\n",
    "            (\n",
    "                (col(\"trip_distance\") < 5) & (col(\"fare_amount\") > 50)\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"week\", date_trunc(\"week\", col(\"tpep_pickup_datetime\")))\n",
    "        .select(\"week\", col(\"pickup_zip\").alias(\"zip\"), \"fare_amount\", \"trip_distance\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "865e6891-f97f-4bc5-af4b-5e4f3b1152dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import date_trunc, avg, col\n",
    "\n",
    "@dlt.table(  # For materialized view, use @dlt.view + mark as materialized in pipeline settings\n",
    "    name=\"weekly_stats\",\n",
    "    comment=\"Silver layer 2: Weekly statistics with average fare and distance\"\n",
    ")\n",
    "def weekly_stats():\n",
    "    df = dlt.read(\"taxi_raw_records\")  # Use read (not read_stream) for materialized view\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"week\", date_trunc(\"week\", col(\"tpep_pickup_datetime\")))\n",
    "          .groupBy(\"week\")\n",
    "          .agg(\n",
    "              avg(\"fare_amount\").alias(\"avg_amount\"),\n",
    "              avg(\"trip_distance\").alias(\"avg_distance\")\n",
    "          )\n",
    "          .orderBy(\"week\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7435436-f262-40b0-90cd-6af993f82ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round as spark_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbcb953a-d2d6-4628-bcb1-72070ea92eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"top_n\",\n",
    "    comment=\"Gold layer: Top N rides to investigate\"\n",
    ")\n",
    "def top_n():\n",
    "    # Reading input tables\n",
    "    flagged_df = dlt.read(\"flagged_rides\")\n",
    "    weekly_df = dlt.read(\"weekly_stats\")\n",
    "\n",
    "    # applying left join and then select\n",
    "    joined_df = (\n",
    "        flagged_df.join(weekly_df, on=\"week\", how=\"left\")\n",
    "        .select(\n",
    "            flagged_df[\"week\"],\n",
    "            spark_round(weekly_df[\"avg_amount\"], 2).alias(\"avg_amount\"),\n",
    "            spark_round(weekly_df[\"avg_distance\"], 3).alias(\"avg_distance\"),\n",
    "            flagged_df[\"fare_amount\"],\n",
    "            flagged_df[\"trip_distance\"],\n",
    "            flagged_df[\"zip\"]\n",
    "        )\n",
    "        .orderBy(flagged_df[\"fare_amount\"].desc())\n",
    "        .limit(10)\n",
    "    )\n",
    "\n",
    "    return joined_df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "4edd8ab7-bf90-4cc4-830b-f549b1697494",
     "origId": 5454096928330848,
     "title": "taxi-python-analysis",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NY Taxi pipeline Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
